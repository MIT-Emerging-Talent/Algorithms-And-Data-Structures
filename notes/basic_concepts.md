## Data Structures

The term data structure refers to a specific method of organizing data for specific types of operations. In other words, a data structure provides a method of arranging all data items that takes into account not just the pieces contained but also their relationships to one another. The phrase data structure refers to the method through which data is stored. We usually think about data structures without getting bogged down in implementation details of certain programming languages or how data is stored in computer memory. This is why abstract mathematical representations of data structures and data types were developed.

* Primitive data structures: fundamental data structures that operate directly on machine instructions.
* Non primitive data structures: more complex, developed from primitive data structures.

### Linear data structure

* arrays.
* stacks
* queues
* linked lists

### Non linear data structure

* graphs
* trees

![ds](https://user-images.githubusercontent.com/37275728/185381435-9335db5b-8c9a-4e74-87dc-deac3c0356f1.png)

## Algorithms
An algorithm is a set of instructions that each have a defined meaning and may be completed in a specified amount of time and effort. 
An algorithm ends after processing a certain amount of instructions, regardless of the input values. 

Requirements of an algorithm:

* Input: the data provided to the algorithm.
* Output: the data generated by an algoithm.
* Definiteness: each instruction must have unabigous interpretation.
* Finiteness: the algorithm can not run ad infinitum. It ends after executing a finite number of steps.
* Effectiveness: each instruction in addition to being specific must be feasible.

### Algorithms as opposed to programs

Programs (for example, operating systems) are exempted from the fourth requirement since an infinite loop may be used to make them run indefinitely. A computer program can, but does not have to, be used to implement an algorithm. They can be described in plain English just as well.

### Diffrent programming paradigms

* <i>Imperative Programming</i> describes computing in terms of instructions that modify the program or data state.
* <i>Declarative Programming</i> specifies what the program should do without describing how to do it.

### What algorithms I need to know?

There are no algorithms that every software engineer must know. 
You read that correctly: you don't need to memorize ten sorting algorithms to be a productive developer. 
If the algorithm is well-known, there is almost certainly a library that already implements it.
That library is most likely well-tested and optimized for maximum performance.
It's pointless to try to reinvent the wheel.
All that you need to know is how to use external libraries and packages.

That being said, learning how known problems were solved is extremely beneficial, especially in the early stages of a programming journey.
We study algorithms not to memorize the implementation so that we can write it down from memory someday, but to learn the programming mindset. 

## Complexity of algorithms

The <b>time complexity</b> of an algorithm is the number of times atomic operations are called while the algorithm is being executed, expressed as a function of the size of a problem.

The <b>space complexity</b> of an algorithm is the space needed by an algorithm to be completed, expressed as a function of the size of a problem.

A problem's size is generally represented as an integer, and this is the number of items that are manipulated.

The function f(n) describing the running time of an algorithm depends not only on the size "n" of the input data but also on the specific data. 
We have the following three categories of cases:

1. Best Case : The best case is the smallest feasible value of f(n).
1. The expected value of f(n) is known as the Average Case.
1. Worst Case: The largest f(n) value for every key conceivable input.

## Rate of Growth

The focus of notations describing the rate of growth is on the general trend of an algorithm.
We don't care if an algorithm takes 1000 or 1012 steps; we only care about how it scales as input size changes. 

### Intuition

The algorithm execution tends to (but it's not always the case) get slower as the input gets larger.
* It takes a constant amount of time to return the first element of an array. If I have an array of 1000 elements and an array of 5000 elements, it should take roughly the same amount of time to access the first element in both cases.
* Summing all the elements of an array becomes linearly longer as the array size grows. If I have a 1000-element array and a 5000-element array, it should take roughly 5 times as long to sum the elements of the second array. 
* Binary search is a logarithmic complexity algorithm. If I have a sorted array of 1000 elements and a sorted array of 10000 elements, it should take roughly twice as long to find an element in the second array. 

### Formal definitions
When analyzing the asymptotic runtime of algorithms, we frequently cannot compute the runtime exactly.
Instead, we estimate it using Big–$O$ and Big–$&#937$.
That is, Big–$O$ is an upper bound on the runtime of a given algorithm, and $Big–&#937$ is a lower bound on the runtime.
This means that the algorithm cannot run slower than its Big–$O$ runtime or faster than its Big–$&#937$ in a particular situation.
However, for some algorithms, the runtime can be precisely computed because it is always the same (therefore $Big–O = Big–&#937 = Big–&#952$).

#### Big–O (Upper Bound)

$f(n) = O(g(n))$, says that the growth rate of $f(n)$ is less than or equal $<=$ that of $g(n)$.

#### Big–&#937; (Lower Bound)
$f(n) = &#937;(g(n))$, says that the growth rate of $f(n)$ is greater than or equal to $>=$ that of $g(n)$.

#### Big–&#952; (Same order)
$f(n) = &#952;(g(n))$, says that the growth rate of $f(n)$ equals $=$ the growth rate of $g(n)$.

If we have $&#952;(n)$, then we also have $O(n)$ and  $&#937;(n)$.

### Rules for using big-O:

1. Ignoring constant factors: $O(c f(n)) = O(f(n))$, where c is a constant; e.g. $O(5n) = O(n)$
1. Ignoring smaller terms: If $a < b$ then $O(a+b) = O(b)$, for example $O(n^2 + n) = O(n^2)$
1. Upper bound only: If $a < b$ then an $O(a)$ algorithm is also an $O(b)$ algorithm. For example, an $O(n)$ algorithm is also an $O(n^2)$ algorithm (but not vice versa).
1. $n$ and $log n$ are "bigger" than any constant, for example $O(n + k) = O(n)$

### Misconceptions

* Outside of academia, you will never be required to perform a formal proof to demonstrate the Big O of an algorithm.
* It is a language used to describe existing algorithms and data structures.
* The fact that an algorithm is O(n) does not imply that it will scale linearly. Only that it is bounded by afun ction of such a form.
* It doesn't say anything about a specific run (which is usually dependent on other factors), only about general trends. So, if given favourably ordered data, an algorithm with worse complexity may outperform a "faster" algorithm.
* Common overengineering practice is attempting to find a better (with smaller complexity) implementation of an existing working function when that function is not even close to being the application's bottleneck. 

## The Running time of a program

One problem can be usually solve in many different ways. When choosing the algorithm we should consider the following:

1. We want an algorithm that is simple to comprehend, develop, and debug.
1. We'd want an algorithm that makes good use of the computer's resources, preferably one that runs as quickly as feasible.

![big_o](https://user-images.githubusercontent.com/37275728/185381461-ec062561-1f55-4cf5-a3fa-d4cc0a2c06df.png)

### Is it possible to develop O(1) algorithm for every problem?

1. In fact, it is not even possible to formulate an algorithm for every problem. Example: https://en.m.wikipedia.org/wiki/Halting_problem
2. Assuming the problem can be solved alogritmically, we can precompute the solutions for each and every possible input and store them in a massive hash table (access is $O(1)$ ). However, for many problems, this is not a feasible solution because the number of single possible inputs may be infinite. We simply do not always know the size of the problem ahead of time. 
3. If precomputing is not an option, then $O(n)$ or even $O(nlogn)$ is the best we can get for many problems. For example, if we are given a list of numbers and we want to return the sum of those numbers, we must access each number in the list. There is no way to do better than $O(n)$. 

### When does an algorithm have O(logn) or O(nlogn) complexity? 

Everyone knows that a single loop has linear complexity and nested loops have quadratic complexity.
How then could we ever manage to achieve logarithmic complexity?
It turns out that things aren't as simple as some tutorials make them out to be. 

Let's take a look at a standard example with a single loop and $O(1)$ complexity:
 
       while n > 0:
           n -= 1

Let's modify it slightly to get $O(logn)$ complexity:

        while n > 0:
           n /= 2

Let's modify it slightly to get $O(nlogn)$ complexity:

        m = n
        while m > 0:
           k = n
           while k > 0:
              k /= 2
           m -= 1


Let's modify it slightly to get $O(log^2n)$ complexity:

        m = n
        while m > 0:
           k = n
           while k > 0:
              k /= 2
           m /= 2
       
