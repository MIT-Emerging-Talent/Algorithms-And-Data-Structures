<h1>Data Structures</h1>

![Data Structures](https://github.com/djeada/Algorithms-And-Data-Structures/blob/master/resources/ds.png)

<h1>Algorithm</h1>
An algorithm is a set of instructions that each have a defined meaning and may be completed in a specified amount of time and effort. 
An algorithm ends after processing a certain amount of instructions, regardless of the input values. 

Requirements of an algorithm:

* Input: the data provided to the algorithm.
* Output: the data generated by an algoithm.
* Definiteness: each instruction must have unabigous interpretation.
* Finiteness: the algorithm can not run ad infinitum. It ends after executing a finite number of steps.
* Effectiveness: each instruction in addition to being specific must be feasible.

Programs (e.g., operating systems) do not have to fulfill the fourth requirement; with an infinite loop, they can run indefinitely.

<h1>Complexity of algorithms</h1>

The <b>time complexity</b> of an algorithm is the time needed by an algorithm to be completed, expressed as a function of the size of a problem.

The <b>space complexity</b> of an algorithm is the space needed by an algorithm to be completed, expressed as a function of the size of a problem.

The function f(n) describing the running time of an algorithm depends not only on the size "n" of the input data but also on the specific data. 
We have the following three categories of cases:

1. Best Case : The best case is the smallest feasible value of f(n).
1. The expected value of f(n) is known as the Average Case.
1. Worst Case: The largest f(n) value for every key conceivable input.


<h1>Rate of Growth</h1>

<h3>Big–OH O (Upper Bound)</h3>
f(n) = O(g(n)), says that the growth rate of f(n) is less than or equal (<=) that of g(n).

<h3>Big–OMEGA &#937; (Lower Bound)</h3>
f(n) = &#937;(g(n)), says that the growth rate of f(n) is greater than or equal to (>=) that of g(n).

<h3>Big–THETA &#952; (Same order)</h3>
f(n) = &#952;(g(n)), says that the growth rate of f(n) equals (=) the growth rate of g(n).

<h3>Rules for using big-O:</h3>

1. Ignoring constant factors: O(c f(n)) = O(f(n)), where c is a constant; e.g. O(5 n) = O(n)
1. Ignoring smaller terms: If a<b then O(a+b) = O(b), for example O(n^2 + n) = O(n^2)
1. Upper bound only: If a<b then an O(a) algorithm is also an O(b) algorithm. For example, an O(n) algorithm is also an O(n^2) algorithm (but not vice versa).
1. n and log n are "bigger" than any constant, for example O(n + k) = O(n)

<h1>The Running time of a program</h1>

One problem can be usually solve in many different ways. When choosing the algorithm we should consider the following:

1. We want an algorithm that is simple to comprehend, develop, and debug.
1. We'd want an algorithm that makes good use of the computer's resources, preferably one that runs as quickly as feasible.
