<h1>Data Structures</h1>

The term data structure refers to a specific method of organizing data for specific types of operations. In other words, a data structure provides a method of arranging all data items that takes into account not just the pieces contained but also their relationships to one another. The phrase data structure refers to the method through which data is stored. We usually think about data structures without getting bogged down in implementation details of certain programming languages or how data is stored in computer memory. This is why abstract mathematical representations of data structures and data types were developed.

* Primitive data structures: fundamental data structures that operate directly on machine instructions.
* Non primitive data structures: more complex, developed from primitive data structures.

<h2>Linear data structure</h2>

* arrays.
* stacks
* queues
* linked lists

<h2>Non linear data structure</h2>

* graphs
* trees


![Data Structures](https://github.com/djeada/Algorithms-And-Data-Structures/blob/master/resources/ds.png)

<h1>Algorithm</h1>
An algorithm is a set of instructions that each have a defined meaning and may be completed in a specified amount of time and effort. 
An algorithm ends after processing a certain amount of instructions, regardless of the input values. 

Requirements of an algorithm:

* Input: the data provided to the algorithm.
* Output: the data generated by an algoithm.
* Definiteness: each instruction must have unabigous interpretation.
* Finiteness: the algorithm can not run ad infinitum. It ends after executing a finite number of steps.
* Effectiveness: each instruction in addition to being specific must be feasible.

<h2>Algorithms as opposed to programs</h2>

Programs (for example, operating systems) are exempted from the fourth requirement since an infinite loop may be used to make them run indefinitely. A computer program can, but does not have to, be used to implement an algorithm. They can be described in plain English just as well.

<h2>Diffrent programming paradigms</h2>

* <i>Imperative Programming</i> describes computing in terms of instructions that modify the program or data state.
* <i>Declarative Programming</i> specifies what the program should do without describing how to do it.

<h1>Complexity of algorithms</h1>

The <b>time complexity</b> of an algorithm is the number of times atomic operations are called while the algorithm is being executed, expressed as a function of the size of a problem.

The <b>space complexity</b> of an algorithm is the space needed by an algorithm to be completed, expressed as a function of the size of a problem.

A problem's size is generally represented as an integer, and this is the number of items that are manipulated.

The function f(n) describing the running time of an algorithm depends not only on the size "n" of the input data but also on the specific data. 
We have the following three categories of cases:

1. Best Case : The best case is the smallest feasible value of f(n).
1. The expected value of f(n) is known as the Average Case.
1. Worst Case: The largest f(n) value for every key conceivable input.


<h1>Rate of Growth</h1>

<h3>Big–OH O (Upper Bound)</h3>
f(n) = O(g(n)), says that the growth rate of f(n) is less than or equal (<=) that of g(n).

<h3>Big–OMEGA &#937; (Lower Bound)</h3>
f(n) = &#937;(g(n)), says that the growth rate of f(n) is greater than or equal to (>=) that of g(n).

<h3>Big–THETA &#952; (Same order)</h3>
f(n) = &#952;(g(n)), says that the growth rate of f(n) equals (=) the growth rate of g(n).

<h3>Rules for using big-O:</h3>

1. Ignoring constant factors: O(c f(n)) = O(f(n)), where c is a constant; e.g. O(5 n) = O(n)
1. Ignoring smaller terms: If a<b then O(a+b) = O(b), for example O(n^2 + n) = O(n^2)
1. Upper bound only: If a<b then an O(a) algorithm is also an O(b) algorithm. For example, an O(n) algorithm is also an O(n^2) algorithm (but not vice versa).
1. n and log n are "bigger" than any constant, for example O(n + k) = O(n)

<h1>The Running time of a program</h1>

One problem can be usually solve in many different ways. When choosing the algorithm we should consider the following:

1. We want an algorithm that is simple to comprehend, develop, and debug.
1. We'd want an algorithm that makes good use of the computer's resources, preferably one that runs as quickly as feasible.

![Run time](https://github.com/djeada/Algorithms-And-Data-Structures/blob/master/resources/big_o.png)
